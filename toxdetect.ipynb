{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twint\n",
      "  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to /tmp/pip-install-qj4b3b6y/twint\n",
      "  Running command git clone -q https://github.com/twintproject/twint.git /tmp/pip-install-qj4b3b6y/twint\n",
      "\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n",
      "  Running command git checkout -q origin/master\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /root/.local/lib/python3.6/site-packages (from twint) (3.7.3)\n",
      "Requirement already satisfied, skipping upgrade: aiodns in /root/.local/lib/python3.6/site-packages (from twint) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /root/.local/lib/python3.6/site-packages (from twint) (4.9.3)\n",
      "Requirement already satisfied, skipping upgrade: cchardet in /root/.local/lib/python3.6/site-packages (from twint) (2.1.7)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from twint) (0.7)\n",
      "Requirement already satisfied, skipping upgrade: elasticsearch in /root/.local/lib/python3.6/site-packages (from twint) (7.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pysocks in /root/.local/lib/python3.6/site-packages (from twint) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from twint) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp_socks in /root/.local/lib/python3.6/site-packages (from twint) (0.5.5)\n",
      "Requirement already satisfied, skipping upgrade: schedule in /root/.local/lib/python3.6/site-packages (from twint) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: geopy in /root/.local/lib/python3.6/site-packages (from twint) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: fake-useragent in /root/.local/lib/python3.6/site-packages (from twint) (0.1.11)\n",
      "Requirement already satisfied, skipping upgrade: googletransx in /root/.local/lib/python3.6/site-packages (from twint) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /root/.local/lib/python3.6/site-packages (from aiohttp->twint) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /root/.local/lib/python3.6/site-packages (from aiohttp->twint) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /root/.local/lib/python3.6/site-packages (from aiohttp->twint) (5.0.2)\n",
      "Requirement already satisfied, skipping upgrade: idna-ssl>=1.0; python_version < \"3.7\" in /root/.local/lib/python3.6/site-packages (from aiohttp->twint) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pycares>=3.0.0 in /root/.local/lib/python3.6/site-packages (from aiodns->twint) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: typing; python_version < \"3.7\" in /root/.local/lib/python3.6/site-packages (from aiodns->twint) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>1.2; python_version >= \"3.0\" in /root/.local/lib/python3.6/site-packages (from beautifulsoup4->twint) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<2,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: python-socks[asyncio]>=1.0.1 in /root/.local/lib/python3.6/site-packages (from aiohttp_socks->twint) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: geographiclib<2,>=1.49 in /root/.local/lib/python3.6/site-packages (from geopy->twint) (1.50)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from googletransx->twint) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.5.0 in /root/.local/lib/python3.6/site-packages (from pycares>=3.0.0->aiodns->twint) (1.14.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->twint) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /root/.local/lib/python3.6/site-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.20)\n",
      "Building wheels for collected packages: twint\n",
      "  Building wheel for twint (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=42071 sha256=029ecb5c24a2a4a415d2004435c12ff89e8ac3fec7fda2472f758e4a0a62011d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-u8ve8vds/wheels/c9/d4/7d/fb0732852f335c92c52450e5c86b6ba5fb09fa226f3584db34\n",
      "Successfully built twint\n",
      "Installing collected packages: twint\n",
      "  Attempting uninstall: twint\n",
      "    Found existing installation: twint 2.1.20\n",
      "    Uninstalling twint-2.1.20:\n",
      "      Successfully uninstalled twint-2.1.20\n",
      "\u001b[33m  WARNING: The script twint is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed twint-2.1.21\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.6/dist-packages (1.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
    "!pip install nest_asyncio    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory china Created \n",
      "Getting 2020-11-24 \n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Getting 2020-11-25 \n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Getting 2020-11-26 \n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Getting 2020-11-27 \n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Getting 2020-11-28 \n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Getting 2020-11-29 \n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Getting 2020-11-30 \n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Getting 2020-12-01 \n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03140166639b47c0a7b3f536dfb67199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b83f6abfea2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"retweets_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import twint\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import os\n",
    "from os import mkdir, path\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def name_cleaning(word):\n",
    "    cleaned = re.sub('[^0-9a-zA-Zㄱ-ㅎ가-힣]', '', word)\n",
    "    return cleaned\n",
    "\n",
    "def twint_search(dirname, keyword, start, end, json_name, limit):\n",
    "    \n",
    "    c = twint.Config()\n",
    "\n",
    "    c.Limit = limit\n",
    "    c.Username = \"realDonaldTrump\"\n",
    "    c.Search = keyword\n",
    "    c.Since = start \n",
    "    c.Until = end\n",
    "    c.Output = json_name\n",
    "    c.Popular_tweets = True\n",
    "    c.Store_json = True\n",
    "    c.Hide_output = True\n",
    "    c.Debug = True\n",
    "    c.Resume = f'{dirname}/save_endpoint/save_endpoint_{start}.txt'\n",
    "\n",
    "    try:\n",
    "        twint.run.Search(c)\n",
    "    \n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        raise\n",
    "\n",
    "    except:\n",
    "        print(f\"Problem with {start}.\")\n",
    "\n",
    "def twint_loop(keyword, start, end, limit=50):\n",
    "    \n",
    "    dirname = name_cleaning(keyword)\n",
    "    \n",
    "    try:\n",
    "        mkdir(dirname)\n",
    "        mkdir(f'{dirname}/save_endpoint')\n",
    "        print(\"Directory\" , dirname ,  \"Created \")\n",
    "    except FileExistsError:\n",
    "        print(\"Directory\" , dirname ,  \"already exists\")\n",
    "       \n",
    "    daterange = pd.date_range(start, end)\n",
    "    \n",
    "    for start_date in daterange:\n",
    "\n",
    "        start = start_date.strftime(\"%Y-%m-%d\")\n",
    "        end = (start_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        json_name = \"\".join(start.split(\"-\")) + \".json\"\n",
    "        json_name = path.join(dirname, json_name)\n",
    "\n",
    "        print(f'Getting {start} ')\n",
    "        twint_search(dirname, keyword, start, end, json_name, limit)\n",
    "\n",
    "Keyword = 'china'\n",
    "twint_loop(Keyword, '2020-11-24', '2020-12-01', limit=50)        \n",
    "\n",
    "DATA_DIR = Path(f\"./{name_cleaning(Keyword)}\") \n",
    "json_files = [pos_json for pos_json in os.listdir(DATA_DIR) if pos_json.endswith('.json')]\n",
    "\n",
    "df_list= []\n",
    "for file_name in tqdm(json_files):\n",
    "    temp_df = pd.read_json(DATA_DIR / file_name, lines=True)\n",
    "    df_list.append(temp_df)\n",
    "    \n",
    "df = pd.concat(df_list, sort=False)\n",
    "df[[\"created_at\", \"id\", \"tweet\", \"retweets_count\"]].tail()\n",
    "\n",
    "#df = pd.read_json('sample.json', lines=True)\n",
    "#df[[\"created_at\", \"id\", \"tweet\", \"retweets_count\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.748 0.761\n",
      "neg\n",
      "<ProbDist with 2 samples>\n",
      "neg\n",
      "0.4431818181818183\n",
      "0.5568181818181815\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import classify, NaiveBayesClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier \n",
    "from sklearn.svm import SVC\n",
    "from random import shuffle\n",
    "\n",
    "stopwords_english = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "\n",
    "emoticons = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';(',':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "   \n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and \n",
    "              word not in emoticons and \n",
    "                word not in string.punctuation): \n",
    "            stem_word = stemmer.stem(word) \n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    "\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    "\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos')) \n",
    "\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))    \n",
    "\n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    " \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    "\n",
    "NBclassifier = NaiveBayesClassifier.train(train_set)\n",
    "NBaccuracy = classify.accuracy(NBclassifier, test_set)\n",
    "\n",
    "SVCclassifier = SklearnClassifier(SVC())\n",
    "SVCclassifier.train(train_set)\n",
    "#SVCclassifier.train(train_set)\n",
    "SVCaccuracy = classify.accuracy(SVCclassifier, test_set)\n",
    "print (NBaccuracy, SVCaccuracy)\n",
    "\n",
    "custom_tweet = \"ching chang chong\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "print (NBclassifier.classify(custom_tweet_set)) \n",
    "\n",
    "prob_result = NBclassifier.prob_classify(custom_tweet_set)\n",
    "print (prob_result) \n",
    "print (prob_result.max()) \n",
    "print (prob_result.prob(\"pos\")) \n",
    "print (prob_result.prob(\"neg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
